########################
 Implementation Details
########################

**************************
 Algorithm Representation
**************************

An algorithm in ReaL is represented as a *dataflow graph* (DFG), where
each node is a *model function call* (MFC, e.g., generate, inference, or
train_step called on an LLM), and each edge specifies the data or
parameter version dependency between nodes.

The dataflow graph of PPO looks like this:

.. figure:: images/dfg/ppo.svg
   :alt: ppodfg
   :align: center

   The dataflow graph of PPO.

Below, we show another example of the `ReMax
<https://arxiv.org/abs/2310.10505>`_, also known as the REINFORCE
algorithm:

.. figure:: images/dfg/reinforce.svg
   :alt: reinforcedfg
   :align: center

   The dataflow graph of ReMax, or REINFORCE.

.. note::

   In this representation, the dataflow graph of pre-training, SFT, or
   reward modeling simplifies to a single train_step node.

More examples can be found in the ``docs/images/dfg`` folder. Only data
dependencies are shown in the figures above. Parameter version
dependencies are enforced so that the first call to a model X at step i
is always preceded by the last call to X at step i-1.

These figures are generated by a standalone script,
``examples/visualize_dfg.py``. For a given algorithm, such as PPO, we
decompose the entire dataflow into individual MFCs, each represented by
a :class:`realhf.MFCDef` object. The input and output keys of each model
function call specify their data dependencies. A list of
:class:`realhf.MFCDef` objects is passed into the ``build_graph``
function to construct a dataflow graph, which is an instance of
``networkx.DiGraph``.

ReaL can efficiently parallelize both intra-node and inter-node
computation across multiple GPUs. Intra-node parallelization is achieved
using the well-known 3D parallelism strategy, commonly employed in
pre-training. Inter-node parallelization involves overlapping or
concurrently executing different nodes on different *device meshes*.

A device mesh D is the unit used for executing an individual function
call. It is defined as a two-dimensional grid of GPUs located in the
cluster. The shape of D is denoted as (N, M), where it covers N nodes
equipped with M devices. Note that device meshes with the same shape
could have different locations.

The i-th MFC node will be executed on the device mesh :math:`D_i`.
**This mesh can either be disjoint from or overlap with the device
meshes of other MFCs.**

.. note::

   We assume that :math:`D_i` either covers several entire hosts or a
   consecutive portion capable of dividing the number of devices on one
   host, e.g., (1, 1), (1, 2), (1, 4), (1, 8), (2, 8), ..., (N, 8) in a
   cluster of (N, 8).

The graph building is performed in ``realhf/api/core/system_api.py``
during the post-initialization of experiment configurations. The
concrete definitions of MFCs in different experiments can be found in
files under ``realhf/experiments/common/``. All experiment
configurations define an ``rpcs`` property, which is first processed by
the ``initial_setup`` method in ``realhf/experiments/common/common.py``
and then passed to the ``ExperimentConfig`` object to build the graph.

************************
 Runtime Infrastructure
************************

ReaL implements a worker-based runtime, consisting of a single
MasterWorker (MasW) on the CPU and multiple ModelWorkers (ModW), each
occupying a separate GPU. For example, in a cluster of (N, 8), there
will be one MasW and N * 8 ModWs.

Overview
========

Recall that MFCs can have independent (either disjoint or overlapping)
device meshes. From the perspective of a ModW or a GPU, it can host one
or more MFCs. The MasW will execute the DFG and send requests to the
corresponding handlers. Each request contains the handler name (e.g.,
Actor or Critic), the interface type (e.g., generate or train_step), and
some metadata (e.g., the input and output keys). Upon receiving the
request, the ModW will locate the corresponding model, run the
computation, and return the results to the MasW to update the
dependency.

Inherited from the base Worker class, both MasW and ModW run the
``_poll`` method inside a while-loop. The ``_poll`` method is their main
task. Outside of the ``_poll`` method, they listen to the controller and
update their internal states, allowing them to be paused, resumed, or
stopped by the controller.

The Procedure of Launching an Experiment
========================================

This section introduces how ReaL launches experiments using local
subprocesses, Ray, or SLURM. Conceptually, the launcher provides similar
functionality to ``torchrun``, but we didn't use ``torchrun`` because
ReaL's code is inherited from the previous SRL project. The scheduler in
SRL can run heterogeneous CPU and GPU tasks, which is difficult to
achieve with ``torchrun``.

.. figure:: images/experiment_workflow.svg
   :alt: exp_workflow

   The execution workflow when launching an experiment with ReaL.

ReaL has two levels of configuration. The outer level is based on the
Hydra structured configuration, as illustrated in the :doc:`quickstart`
section. This level abstracts an experiment into several configurable
fields, allowing the user to conveniently change hyperparameters such as
the parallelism strategy, learning rate, and batch size.

Next, ReaL translates the Hydra configuration into a worker-based
configuration. This includes the dataset, model, interface, and backend
configurations for each ModW. For concrete examples, please refer to
``realhf/api/core/config.py``. The core translation code is written in
the ``_get_model_worker_configs`` method in
``realhf/experiments/common/common.py``. This configuration level
retains maximum flexibility. For instance, if we need to run CPU-heavy
tasks like a reward function, we can implement a customized worker to
execute the task on CPUs.

The worker configuration is registered as an "experiment" with a unique
name in ``realhf/api/quickstart/entrypoint.py``. It is then launched by
``realhf.apps.main``. The launcher finds the experiment by its name,
loads the worker configurations, and submits them to the scheduler
(either SLURM or local subprocesses). The scheduler runs a worker
controller to manage the lifetime of other workers. Workers continuously
check for new messages from the controller and change their internal
state (e.g., running, pausing, or stopping) accordingly. Once the
controller determines that all ModWs and the MasW are ready, it sends a
signal to all workers to start the experiment. If the scheduler detects
that a worker is no longer alive, such as after the experiment is
completed or if an unexpected error occurs, it will shut down the
controller and all workers, and exit ``realhf.apps.main``.

Model, Model Interface, and Model Backend
=========================================

A :class:`realhf.Model` is a collection that includes a
transformer-based neural network, a HuggingFace tokenizer, and some
metadata, all associated with a unique name. The ``module`` attribute is
usually a ``ReaLModel`` before backend initialization, and it becomes a
:class:`realhf.PipelinableEngine` after backend initialization. The
``module`` can be a shard of parameters or even an empty placeholder
when offloading or parameter reallocation is enabled.

A :class:`realhf.ModelInterface` is a collection of concrete
implementations for generation, inference, and training. When the MasW
requests a specific MFC, the ModW will find the correct
:class:`realhf.Model` and pass it into the configured algorithm
interface for execution. The results returned by the interface are then
sent back to the MasW. This is implemented in the
``__handle_model_function_calls`` method in
``realhf/system/model_worker.py``.

.. note::

   Even though the computational workloads can be categorized into these
   main types, different algorithms often have unique side-effects. For
   example, PPO requires computing the GAE during training, while DPO
   does not. Therefore, we implement interfaces for each algorithm to
   facilitate easier customization.

.. note::

   It doesn't need to implement all interface types; for example, an
   interface for SFT only needs to implement the train_step method.

A :class:`realhf.ModelBackend` is a functor that wraps the
:class:`realhf.Model` to provide additional functionalities like
pipelined inference and ZeRO optimizer. It changes the ``module``
attribute of the :class:`realhf.Model` to a
:class:`realhf.PipelinableEngine` object. All interface implementations
use the APIs of :class:`realhf.PipelinableEngine` to run the main
computation. See ``realhf/impl/model/interface`` for concrete examples.

Once launched, the ModW will set up all configured models, interfaces,
and backends (see the ``__lazy_setup`` method in
``realhf/system/model_worker.py``). They are indexed by the unique names
of the :class:`realhf.Model`. In the ModW, a :class:`realhf.MFCDef`, a
:class:`realhf.Model`, a :class:`realhf.ModelInterface`, and a
:class:`realhf.ModelBackend` are bound togather to handle a specific
MFC, either generate, inference, or train_step.

.. note::

   Algorithm customization typically involves implementing a new
   interface. For example, a customized reward interface is shown in
   ``examples/customized_exp/ppo_sentiment.py``.

MasW-ModW Communication
=======================

The request-reply communication between the MasW and ModWs is managed
through ZMQ sockets. We abstract the communication pattern in
``realhf/system/request_reply_stream.py``. The communication channel is
set up in the ``__lazy_setup`` method in both types of workers. The
communication is lightweight, as we only transfer metadata between them,
such as the keys and IDs of the input and output tensors.

We adopt a TCP-like protocol to ensure that all involved ModWs receive
the request simultaneously. Requests are pushed into a queue in the ModW
and handled sequentially. In addition to MFCs, requests can also include
initialization, data fetching, saving, evaluation, etc. For more
details, see the ``model_poll_step`` and ``_poll`` methods in
``realhf/system/model_worker.py``.

Data Transfer
=============

The dataset resides on the ModWs responsible for handling the source MFC
in the DFG. For example, in PPO, the dataset is stored in the ModWs that
handle actor generation. The dataset is sharded across different data
parallel ranks. See the ``__lazy_setup`` function in ModW for details.

At the start of each epoch, the MasW will continuously send data
fetching requests to the ModWs until the dataset has been fully
iterated. The ModWs will step through the dataloader and return metadata
(e.g., sequence length, keys in the dataset, IDs, etc.) to the MasW. The
MasW will fill an internal buffer with this metadata.

MasW's buffer tracks how many times each piece of data has been used in
the DFG, and which keys have been produced by MFCs. Once the dependency
of an MFC is satisfied—i.e., the required input keys are all available
in the buffer— the MasW will send a request to the corresponding ModWs
to run the MFC. If the MFC produces new keys, the resulting GPU tensors
will be stored locally, and the ModWs will send metadata back to the
MasW to update the buffer. After a piece of data has been used by all
nodes in the DFG, it will be removed.

If the buffer size is insufficient for subsequent operations, the MasW
will send data fetching requests to the ModWs for the next epoch. These
behaviors are implemented in the ``load_data_func`` in MasW, the
``prefetch_from_dataset`` and ``model_poll_step`` methods in ModW, and
``realhf/system/buffer.py``.

Data is replicated across tensor and pipeline parallel dimensions and
sharded across the data parallel dimension. Since different MFCs may
have different device meshes and parallel strategies, we need to
transfer data from the owner (or producer) to the consumer before MFC
computation. This is implemented as **hooks** in requests. Since the
MasW maintains global information, it can append the source and
destination of the required data in the pre-hooks and send them to the
relevant ModWs. The ModW will then trigger GPU-GPU data transfer via a
broadcast-based algorithm to properly retrieve all the required data.
This is implemented in the ``__handle_one_rpc_hook`` method in ModW.

Parameter Reallocation
======================

ReaL automatically reallocates model parameters to peer GPUs or CPU
memory to reduce GPU memory usage and the communication volume caused by
parallelization. However, there is an implementation-specific detail to
note: if a model is being trained, its parameter memory cannot be
released after reallocation. This is because the PyTorch optimizer
(e.g., Adam) keeps model parameters as dictionary keys, and GPU tensor
handles remain active.

Due to this limitation, we must categorize models as either trainable or
non-trainable. If any MFC involves training the model, the model is
categorized as trainable. For example, in PPO, the actor and critic are
trainable, while the reward and reference models are not.

For non-trainable models, we can safely reallocate their parameters to
CPU memory (i.e., offloading). The parameters will be asynchronously
transferred back (i.e., overlapping computation and communication) to
GPU memory during the next forward pass. When multiple inference
requests are made for the same role, each request will have its own copy
of the parameters and will be offloaded independently. Offloading is
implemented in the ``async_offload`` method in ``ReaLModel``, which is
called in the ``__handle_one_rpc_hook`` method in ModW.

For trainable models, if there is also an inference or generate MFC
called upon this role (e.g., Actor and Critic in PPO), we can adopt
different parallel strategies for different MFCs and dynamically
reallocate parameters to reduce communication overhead. The training MFC
holds its own parameters in GPU memory, while non-training MFCs only
hold empty placeholders. When a non-training MFC is requested, the MasW
will append a pre-hook to the request containing all the information for
reallocating the parameters, and a post-hook to revert this operation.
The reallocation is implemented in the ``__handle_one_rpc_hook`` method
in ModW. Note that since the trainable parameters cannot be released,
the reverse reallocation essentially drops the parameters used for
inference or generation.

.. note::

   The above limitation of PyTorch is not an intrinsic problem. We could
   re-implement the optimizer and use parameter names as keys. However,
   this would require modifying Megatron and DeepSpeed correspondingly,
   which is not a trivial task.

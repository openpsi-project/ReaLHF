
<!DOCTYPE html>
<html class="no-js" lang="en">
  <head>
    <meta charset="utf-8" />
      <meta name="viewport" content="width=device-width, initial-scale=1" />

    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="color-scheme" content="light dark" />
    
    <meta name="docsearch:name" content="ReaL" />
    <meta name="docsearch:package_type" content="" />
    <meta name="docsearch:release" content="0.3.0" />
    <meta name="docsearch:version" content="" />
    
      <title>Quickstart &mdash; ReaL 0.3.0 documentation</title>
    
    <link rel="canonical" href="quickstart" />
          <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=8e8a900e" /><!-- add (1) -->
          <link rel="stylesheet" type="text/css" href="_static/custom.css?v=97da1bf0" /><!-- add (1) -->
          <link rel="stylesheet" type="text/css" href="_static/sphinx-nefertiti-default.min.css?v=0f5ffd77" /><!-- add (1) -->
          <link rel="stylesheet" type="text/css" href="_static/fonts/nunito/stylesheet.css?v=0ed606bc" /><!-- add (1) -->
          <link rel="stylesheet" type="text/css" href="_static/fonts/red-hat-mono/stylesheet.css?v=4eee5046" /><!-- add (1) -->
          <link rel="stylesheet" type="text/css" href="_static/pygments-dark.css?v=2de69186" /><!-- add (1) -->
          <link rel="stylesheet" type="text/css" href="_static/pygments-light.css?v=970f37b1" /><!-- add (1) -->
          <link rel="stylesheet" type="text/css" href="_static/bootstrap-icons.min.css?v=44730005" /><!-- add (1) -->
        <link rel="index" title="Index" href="genindex.html" />
        <link rel="search" title="Search" href="search.html" />
        <link rel="top" title="ReaL 0.3.0 documentation" href="#" />
        <link rel="next" title="Setting Up Distributed Experiments" href="distributed.html" />
        <link rel="prev" title="Configurations" href="expconfig.html" />
    <style>
      :root {
        --nftt-body-font-family: "Nunito", var(--nftt-font-sans-serif) !important;
        --nftt-font-monospace: "Red Hat Mono", var(--nftt-font-family-monospace) !important;
        --nftt-project-name-font: var(--nftt-body-font-family);
        --nftt-documentation-font: var(--nftt-body-font-family);
        --nftt-doc-headers-font: "Georgia", var(--nftt-documentation-font);}
      h1 *, h2 *, h3 *, h4 *, h5 *, h6 * { font-size: inherit; }
    </style>
  </head>
  <body>
    <div id="back-to-top-container" class="position-fixed start-50 translate-middle-x">
      <button id="back-to-top" type="button" class="d-none btn btn-neutral btn-sm shadow px-4" data-bs-toggle="button">Back to top</button>
    </div>
    <header id="snftt-nav-bar" class="navbar navbar-expand-xl navbar-dark nftt-navbar flex-column fixed-top">
      <div class="skip-links container-fluid visually-hidden-focusable overflow-hidden justify-content-start flex-grow-1">
        <div class="border-bottom mb-2 pb-2 w-100">
          <a class="d-none d-md-inline-flex p-2 m-1" href="#sidebar-filter">Skip to docs navigation</a>
          <a class="d-inline-flex p-2 m-1" href="#content">Skip to main content</a>
        </div>
      </div>
      <nav class="container-xxl nftt-gutter flex-wrap flex-xl-nowrap" aria-label="Main navigation">
        <div class="nftt-navbar-toggler">
          <button class="navbar-toggler p-2" type="button" data-bs-toggle="offcanvas" data-bs-target="#sidebar" aria-controls="sidebar" aria-label="Toggle documentation navigation">
            <i class="bi bi-list"></i>
          </button>
        </div>
          <a href="index.html"
              
              class="navbar-brand p-0 me-0 md-lg-2 pe-lg-4"
          ><span class="brand-text">ReaL</span></a>
        
        
        <div class="d-flex d-xl-none">
          <button class="navbar-toggler p-2" type="button" data-bs-toggle="offcanvas" data-bs-target="#nfttSearch" aria-controls="nfttSearch" aria-label="Search">
            <i class="bi bi-search"></i>
          </button>
          <button class="navbar-toggler p-2" type="button" data-bs-toggle="offcanvas" data-bs-target="#nfttNavbar" aria-controls="nfttNavbar" aria-label="Toggle navigation">
            <i class="bi bi-three-dots"></i>
          </button>
        </div>
        
<div class="offcanvas-xl offcanvas-end flex-grow-1" tabindex="-1" id="nfttSearch" aria-labelledby="nfttSearchOffcanvasLabel" data-bs-scroll="true">
  <div class="offcanvas-header px-4 pb-0">
    <h5 class="offcanvas-title fw-bold" id="nfttSearchOffcanvasLabel">Search the documentation</h5>
    <button type="button" class="btn-close" data-bs-dismiss="offcanvas" aria-label="Close" data-bs-target="#nfttSearch"></button>
  </div>
  <div class="offcanvas-body p-4 pt-0 p-xl-0 ps-xl-4">
    <hr class="d-xl-none text-white-50">
    <ul class="navbar-nav flex-row align-items-center flex-wrap ms-md-auto">
      <li class="nav-item col-12 col-xl-auto">
        <form id="nftt-search-form" action="search.html" method="get">
          <div class="input-group">
            <input type="text" name="q" class="form-control search-input" placeholder="Search docs" aria-label="Search" aria-describedby="button-search">
            <input type="hidden" name="check_keywords" value="yes" />
            <input type="hidden" name="area" value="default" />
            <button class="btn btn-primary" type="submit" id="button-search" aria-label="Search"><i class="bi bi-search"></i></button>
          </div>
        </form>
      </li>
    </ul>
  </div>
</div>

        <div class="offcanvas-xl offcanvas-end" tabindex="-1" id="nfttNavbar" aria-labelledby="nfttNavbarOffcanvasLabel" data-bs-scroll="true">
          <div class="offcanvas-header px-4 pb-0">
            <div class="offcanvas-title navbar-brand" id="nfttNavbarOffcanvasLabel"><span class="brand-text">ReaL</span></div>
            <button type="button" class="btn-close btn-close-white" data-bs-dismiss="offcanvas" aria-label="Close" data-bs-target="#nfttNavbar"></button>
          </div>
          <div class="offcanvas-body p-4 pt-0 p-xl-0 px-xl-3">
            <hr class="d-xl-none text-white-50">
            <ul class="navbar-nav flex-row align-items-center flex-wrap ms-lg-auto">
              
              
              
              
              <!-- colorscheme_dropdown.html -->
<li class="nav-item dropdown">
  <a class="nav-link d-flex py-2 px-0 px-xl-2 dropdown-toggle align-items-center" id="snftt-luz" href="#" data-bs-toggle="dropdown" data-bs-display="static" aria-expanded="false" aria-label="Toggle light/dark">
    <i class="bi bi-circle-half" data-snftt-luz-icon-active></i>
    <span id="snftt-luz-text" class="d-xl-none ms-2">Toggle light/dark</span>
  </a>
  <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="snftt-luz-text">
    <li>
      <h6 class="dropdown-header">Light/dark</h6>
    </li>
    <li>
      <a class="dropdown-item d-flex align-items-center" data-snftt-luz="light" href="#" aria-pressed="false">
        <span>
          <i class="bi bi-sun" data-snftt-luz-icon="light"></i>
        </span>
        <span class="ms-3">Light</span>
        <i class="bi bi-check ms-auto"></i>
      </a>
    </li>
    <li>
      <a class="dropdown-item d-flex align-items-center" data-snftt-luz="dark" href="#" aria-pressed="false">
        <span>
          <i class="bi bi-moon-stars" data-snftt-luz-icon="dark"></i>
        </span>
        <span class="ms-3">Dark</span>
        <i class="bi bi-check ms-auto"></i>
      </a>
    </li>
    <li>
      <a class="dropdown-item current d-flex align-items-center" data-snftt-luz="default" href="#" aria-pressed="false">
        <span>
          <i class="bi bi-circle-half" data-snftt-luz-icon="default"></i>
        </span>
        <span class="ms-3">Default</span>
        <i class="bi bi-check ms-auto"></i>
      </a>
    </li>
  </ul>
</li>
            </ul>
          </div>
        </div>
      </nav>
      
    </header>

    <div class="container-fluid flex-grow-1">
      <div class="nftt-gutter nftt-page">
        <aside class="nftt-sidebar ">
          <div class="nftt-sidebar-content">
            
            <div class="title d-none d-xl-block">
              <i class="bi bi-book"></i>&nbsp;&nbsp;<span>Index</span>
            </div>
            <div id="sidebar" tabindex="-1" class="offcanvas-xl offcanvas-start" aria-labelledby="nfttSidebarOffcanvasLabel">
                <!-- sidebartemplate: "globaltoc.html" --><div class="offcanvas-header border-bottom">
  <h5 class="offcanvas-title fw-bold" id="nfttSidebarOffcanvasLabel">
    Table of contents
  </h5>
  <button type="button" class="btn-close" data-bs-dismiss="offcanvas" aria-label="Close" data-bs-target="#sidebar"></button>
</div>

<div class="offcanvas-body">
  <nav class="toc" aria-label="Main menu">
    <div class="mb-3 p-1 pt-3 pb-4 border-bottom">
      <input id="sidebar-filter" type="text" name="filter" class="form-control form-control-sm" placeholder="filter" aria-label="filter">
    </div>
    <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="intro.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="install.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="expconfig.html">Configurations</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Quickstart</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.html">Setting Up Distributed Experiments</a></li>
<li class="toctree-l1"><a class="reference internal" href="customization.html">Customization</a></li>
<li class="toctree-l1"><a class="reference internal" href="impl.html">Implementation Details</a></li>
<li class="toctree-l1"><a class="reference internal" href="arch.html">Code Architecture</a></li>
<li class="toctree-l1"><a class="reference internal" href="contributing.html">Contributing</a></li>
</ul>

  </nav>
  <template data-toggle-item-template>
    <button class="btn btn-sm btn-link toctree-expand" type="button">
      <i class="bi bi-caret-right"></i>
      <span class="visually-hidden">Toggle menu contents</span>
    </button>
  </template>
</div>
            </div>
            
          </div>
        </aside>
        <article id="content" class="nftt-content" role="main">
          <nav aria-label="breadcrumb">
  <ol class="breadcrumb">
    <li class="breadcrumb-item"><a href="index.html">Start</a></li>
    <li class="breadcrumb-item active" aria-current="page">Quickstart</li>
  </ol>
</nav>
    <section id="quickstart">
<h1>Quickstart<a class="headerlink" href="#quickstart" title="Link to this heading">¶</a></h1>
<section id="installation">
<h2>Installation<a class="headerlink" href="#installation" title="Link to this heading">¶</a></h2>
<p>First, clone the ReaL repository from GitHub and follow the installation
instructions in <a class="reference internal" href="install.html"><span class="doc">Installation</span></a>.</p>
</section>
<section id="rlhf-with-4x-llama-7b-in-30min">
<h2>RLHF with 4x LLaMA-7B in 30min<a class="headerlink" href="#rlhf-with-4x-llama-7b-in-30min" title="Link to this heading">¶</a></h2>
<p>If you are not familiar with the procedure of RLHF, please refer to the
<a class="reference external" href="https://arxiv.org/abs/2203.02155">InstrctGPT paper</a>. This tutorial
will cover the main stages of RLHF, including SFT, reward modeling, and
PPO.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you have not prepared a dataset for your application, you can
download our <a class="reference external" href="https://drive.google.com/drive/folders/1xWIJ9DRLNQZxDrkCfAPE12euLLuWQGE-?usp=sharing">sample dataset</a>
to follow this tutorial. The sample dataset is used for controlled
sentiment generation, where the LLM learns to generate positive movie
comments given a context.</p>
<p>All example scripts with detailed explanations can be found in the
<code class="docutils literal notranslate"><span class="pre">examples/scripts/</span></code> directory.</p>
<p>If you want to find more details about the configuration options,
please check <a class="reference internal" href="expconfig.html"><span class="doc">Configurations</span></a>.</p>
</div>
<p>Suppose the dataset has been placed under the <code class="docutils literal notranslate"><span class="pre">.data/</span></code> folder, now you
are ready to run the RLHF process!</p>
<section id="stage-1-supervised-fine-tuning">
<h3>Stage 1: Supervised Fine-Tuning<a class="headerlink" href="#stage-1-supervised-fine-tuning" title="Link to this heading">¶</a></h3>
<p>Prepare your customized dataset in a JSON or JSONL format, where each
entry is a dictionary with two keys: “prompt” and “answer”. For example:</p>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="p">{</span><span class="nt">&quot;prompt&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;What is the capital of France?&quot;</span><span class="p">,</span><span class="w"> </span><span class="nt">&quot;answer&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;The capital of France is ...&quot;</span><span class="p">}</span>
<span class="p">{</span><span class="nt">&quot;prompt&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;Please make a travel plan for visiting Berlin?&quot;</span><span class="p">,</span><span class="w"> </span><span class="nt">&quot;answer&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;...&quot;</span><span class="p">}</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In our provided sample, <code class="docutils literal notranslate"><span class="pre">sft_pos-train.jsonl</span></code> and
<code class="docutils literal notranslate"><span class="pre">sft_pos-valid.jsonl</span></code> are the training and validation sets for SFT,
respectively.</p>
</div>
<p>Run the command in <a class="reference external" href="https://github.com/openpsi-project/ReaLHF/blob/main/examples/scripts/local/sft.sh">examples/local/sft.sh</a>
to fine-tune the model on your dataset.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>ReaL adopts <a class="reference external" href="https://hydra.cc/docs/tutorials/structured_config/intro/">structured configurations</a> in
<a class="reference external" href="https://hydra.cc/">Hydra</a> to manage command line options. The
options in the above command correspond to a Python dataclass object:
<a class="reference internal" href="expconfig.html#realhf.SFTConfig" title="realhf.SFTConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">realhf.SFTConfig</span></code></a>. The attributes, including the model type,
learning rate, and parallel strategy, can be recursively overwritten
via command line options. Please check <a class="reference internal" href="expconfig.html"><span class="doc">Configurations</span></a> to see all
the fields that can be overwritten.</p>
<p>As a reminder, the value <cite>null</cite> in the Hydra option represents <cite>None</cite>
in Python.</p>
</div>
<p>The user can specify the number of nodes and the parallel strategy to
use with the above command, in addition to paths and hyperparameters. In
the given example, the experiment will use 1 node (assuming each node
has 8 GPUs, implicitly set by the <code class="docutils literal notranslate"><span class="pre">n_gpus_per_node</span></code> attribute), with a
parallel strategy (pipe=1, tensor=2, data=4) and a batch size of 512.</p>
<p>After the experiment has been successfully launched, you will see the
training logs in the console like this:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">20240618-03:10:56.216 quickstart INFO: Running sft experiment.</span>
<span class="go">20240618-03:10:56.216 quickstart INFO: Logs will be dumped to /lustre/aigc/llm/logs/fw/quickstart-sft/release</span>
<span class="go">20240618-03:10:56.216 quickstart INFO: Model checkpoints will be saved to /lustre/aigc/llm/checkpoints/fw/quickstart-sft/release</span>
<span class="go">...</span>
</pre></div>
</div>
<p>The above output shows the log and checkpoint paths of this experiment,
according to the given <code class="docutils literal notranslate"><span class="pre">experiment_name</span></code> and <code class="docutils literal notranslate"><span class="pre">trial_name</span></code>. You can
check the logs and overrided configurations:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>ls<span class="w"> </span>/lustre/aigc/llm/logs/fw/quickstart-sft/release/
<span class="go">ctl-0            master_worker-0  time_marks0.pkl  time_marks2.pkl  time_marks4.pkl  time_marks6.pkl</span>
<span class="go">hydra-outputs/   model_worker-0   time_marks1.pkl  time_marks3.pkl  time_marks5.pkl  time_marks7.pkl</span>
<span class="gp">$ </span><span class="c1"># Check the training statistics like loss and running time in the master worker.</span>
<span class="gp">$ </span>cat<span class="w"> </span>/lustre/aigc/llm/logs/fw/quickstart-sft/release/master_worker-0
<span class="gp">$ </span><span class="c1"># Check the runtime system metrics in the model worker.</span>
<span class="gp">$ </span>cat<span class="w"> </span>/lustre/aigc/llm/logs/fw/quickstart-sft/release/model_worker-0
<span class="gp">$ </span><span class="c1"># Check the adopoted configurations.</span>
<span class="gp">$ </span>cat<span class="w"> </span>/lustre/aigc/llm/logs/fw/quickstart-sft/release/hydra-outputs/.hydra/overrides.yaml
</pre></div>
</div>
<p>You can also check the checkpoints:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>ls<span class="w"> </span>/lustre/aigc/llm/checkpoints/fw/quickstart-sft/release/default/epoch7epochstep5globalstep50/
<span class="go">config.json                       pytorch_model-00007-of-00014.bin  pytorch_model-00014-of-00014.bin</span>
<span class="go">pytorch_model-00001-of-00014.bin  pytorch_model-00008-of-00014.bin  pytorch_model.bin.index.json</span>
<span class="go">pytorch_model-00002-of-00014.bin  pytorch_model-00009-of-00014.bin  special_tokens_map.json</span>
<span class="go">pytorch_model-00003-of-00014.bin  pytorch_model-00010-of-00014.bin  tokenizer.json</span>
<span class="go">pytorch_model-00004-of-00014.bin  pytorch_model-00011-of-00014.bin  tokenizer.model</span>
<span class="go">pytorch_model-00005-of-00014.bin  pytorch_model-00012-of-00014.bin  tokenizer_config.json</span>
<span class="go">pytorch_model-00006-of-00014.bin  pytorch_model-00013-of-00014.bin</span>
</pre></div>
</div>
<p>Here, <code class="docutils literal notranslate"><span class="pre">default</span></code> is the model name. Since we would save multiple models
for algorithms like PPO, the model name is used to distinguish different
models. SFT has a single model named <code class="docutils literal notranslate"><span class="pre">default</span></code>.</p>
<p>The directory suffix indicates the step of this checkpoint. It’s the
checkpoint after 50 training steps at step 5 of epoch 7 (we have set
<code class="docutils literal notranslate"><span class="pre">save_freq_steps=50</span></code>). You can change the save and evaluation
frequency by modifying the <code class="docutils literal notranslate"><span class="pre">exp_ctrl</span></code> attribute in
<a class="reference internal" href="expconfig.html#realhf.SFTConfig" title="realhf.SFTConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">realhf.SFTConfig</span></code></a>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>ReaL directly loads from HuggingFace models and also saves
checkpoints as HuggingFace models, making it convenient to use
pre-trained checkpoints and to deploy trained models with inference
frameworks like vLLM.</p>
<p>You can directly pass the path of the above checkpoint to
<code class="docutils literal notranslate"><span class="pre">transformers.AutoModelForCausalLM.from_pretrained</span></code> or vLLM to load
the model.</p>
</div>
<img alt="_images/sft_loss.svg" class="align-center" src="_images/sft_loss.svg" /><div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>cat<span class="w"> </span>/lustre/aigc/llm/logs/fw/quickstart-sft/release/master_worker-0
<span class="go">...</span>
<span class="go">0: 20240618-13:32:19.081 master worker INFO: Execution finished!</span>
<span class="go">0: 20240618-13:32:19.083 master worker INFO: Epoch 8/8 step 7/7 ... Total time consumption: 628.051s. ...</span>
<span class="go">...</span>
<span class="go">0: 20240618-13:32:34.906 master worker INFO: Execution finished!</span>
<span class="go">0: 20240618-13:32:34.906 master worker INFO: Training complete! Yeah!!!</span>
</pre></div>
</div>
<p>The SFT experiment will take about 10 minutes to finish using our
provided dataset and configuration. Let’s move on to the next stage.</p>
</section>
<section id="stage-2-1-reward-modeling-rm">
<h3>Stage 2.1: Reward Modeling (RM)<a class="headerlink" href="#stage-2-1-reward-modeling-rm" title="Link to this heading">¶</a></h3>
<p>Prepare your customized dataset in a JSON or JSONL format, where each
entry is a dictionary with three keys: “prompt”, “pos_answers”, and
“neg_answers”.</p>
<p>“prompt” should be a string, while “pos_answers” and “neg_answers”
should be lists of strings of the same size, forming pairwise
comparisons.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In our provided sample, <code class="docutils literal notranslate"><span class="pre">rm_paired-train.jsonl</span></code> and
<code class="docutils literal notranslate"><span class="pre">rm_paired-valid.jsonl</span></code> are the training and validation sets for
reward modeling, respectively.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>“pos_answers” and “neg_answers” may contain duplicated data. For
example, if a prompt has four answers and they have pairwise
comparisons, the length of “pos_answers” and “neg_answers” should be
six, and each answer will appear in three pairs.</p>
</div>
<p>Run <a class="reference external" href="https://github.com/openpsi-project/ReaLHF/blob/main/examples/scripts/local/rw.sh">examples/local/rw.sh</a>
to train the reward model.</p>
<p>It’s a common practice to use the SFT model to initialize the reward
model. Therefore, we can pass the path of the saved SFT model as
<code class="docutils literal notranslate"><span class="pre">model.path</span></code>. Using the pre-trained LLaMA checkpoint is also feasible,
but it may not perform as well as the SFT checkpoint.</p>
<p>The output head of the loaded LLM will be replaced by a newly
initialized linear layer, which outputs a scalar as the reward.</p>
<p>In reward modeling, the batch size is the number of prompts. With a
batch size of 512, there will be at most 512 * max_pairs_per_prompt
positive samples and 512 * max_pairs_per_prompt negative samples in each
batch.</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>bash<span class="w"> </span>examples/scripts/rw.sh
<span class="go">0: 20240618-13:52:00.094 master worker INFO: Running rw experiment.</span>
<span class="go">0: 20240618-13:52:00.094 master worker INFO: Logs will be dumped to /lustre/aigc/llm/logs/fw/quickstart-rw/release</span>
<span class="go">0: 20240618-13:52:00.094 master worker INFO: Model checkpoints will be saved to /lustre/aigc/llm/checkpoints/fw/quickstart-rw/release</span>
<span class="go">...</span>
</pre></div>
</div>
<p>The log and checkpoint paths are similar to that of SFT, except that the
experiment name and trial name can be changed. Note that the saved RW
checkpoint is not loadable by HuggingFace or vLLM, because the
projection head has been changed.</p>
<p>Please check <a class="reference external" href="https://github.com/openpsi-project/ReaLHF/blob/main/examples/load_and_eval_rw.py">examples/load_and_eval_rw.py</a>
as an example to load and use the trained reward model in a standalone
script.</p>
<img alt="_images/rw_loss.svg" class="align-center" src="_images/rw_loss.svg" /><p>Training the reward model to convergence can be very fast. In the given
example, we can stop the training after 15 steps, which takes
approximately 5 minutes.</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">0: 20240618-13:53:00.094 master worker INFO: Epoch 1/1 step 15/26 (global step 15) finishes. ... Total time consumption: 294.393s.</span>
</pre></div>
</div>
</section>
<section id="stage-2-2-direct-preference-optimization-dpo">
<h3>Stage 2.2: Direct Preference Optimization (DPO)<a class="headerlink" href="#stage-2-2-direct-preference-optimization-dpo" title="Link to this heading">¶</a></h3>
<p>Besides the ordinary RLHF procedure with PPO, ReaL also supports the
<a class="reference external" href="https://arxiv.org/abs/2305.18290">DPO algorithm</a>, which avoids reward
modeling.</p>
<p>The dataset for DPO is exactly the same as for reward modeling. Run
<a class="reference external" href="https://github.com/openpsi-project/ReaLHF/blob/main/examples/scripts/local/dpo.sh">examples/local/dpo.sh</a>
to train DPO.</p>
<p>Note that there’s a major difference between DPO and SFT/RM. DPO
involves <strong>two</strong> different models, the <em>actor</em> and the <em>reference</em>. The
former is the primary LLM to be trained and the latter is the frozen SFT
model to provide KL regularizations.</p>
<p>A training iteration of DPO is composed of two steps:</p>
<ul class="simple">
<li><p><em>RefInf</em>: The reference model performs a forward step to compute the
log probabilities of positive and negative answers.</p></li>
<li><p><em>ActorTrain</em>: Given the reference log probabilities, the actor model
computes the DPO loss, runs the backward pass, and updates the
parameters.</p></li>
</ul>
<p>In ReaL, these two steps can run with different parallel strategies,
maximizing the efficiency of the individual workloads. These parallel
strategies can be specified in the <code class="docutils literal notranslate"><span class="pre">ref_inf</span></code> and <code class="docutils literal notranslate"><span class="pre">actor_train</span></code>
fields. Under a moderate batch size, pipelined inference can be faster
than tensor-paralleled inference due to the reduced communication
overhead, so assigning a relatively large <code class="docutils literal notranslate"><span class="pre">pipeline_parallel_size</span></code> for
<code class="docutils literal notranslate"><span class="pre">ref_inf</span></code> can be favorable.</p>
<p>Moreover, ReaL can automatically <em>offload</em> the parameters of the
reference model once <em>RefInf</em> is done. This offloading fully supports 3D
parallelism and does not require DeepSpeed ZeRO-3 or any additional
configurations. Consequently, <strong>ReaL’s DPO is as memory-efficient as
training a single model like SFT!</strong></p>
</section>
<section id="stage-3-ppo">
<h3>Stage 3: PPO<a class="headerlink" href="#stage-3-ppo" title="Link to this heading">¶</a></h3>
<p>After the SFT and RM stages, we can proceed to the PPO stage. The
dataset for PPO should be a JSON or JSONL file with each entry being a
dictionary with a single key “prompt”.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In our provided sample, <code class="docutils literal notranslate"><span class="pre">ppo_prompt.jsonl</span></code> is the training set for
PPO.</p>
</div>
<p>Run <a class="reference external" href="https://github.com/openpsi-project/ReaLHF/blob/main/examples/scripts/local/ppo.sh">examples/local/ppo.sh</a>
to train PPO.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>You can also pass in the trained DPO checkpoint to initialize the PPO
actor.</p>
<p>If you want to direct initialize the critic model with a random
projection head, you can set <code class="docutils literal notranslate"><span class="pre">critic.init_critic_from_actor=True</span></code>
and <code class="docutils literal notranslate"><span class="pre">reward.init_critic_from_actor=True</span></code>. This is helpful for
benchmarking throughputs since checkpoints of SFT and reward modeling
are not required.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The provided PPO command sets <code class="docutils literal notranslate"><span class="pre">ppo.gen.min_new_tokens=512</span></code>. This is
for benchmarking purposes and should be changed to 0 for normal
training.</p>
</div>
<p>The configuration options of PPO are the most complex among the three
stages. PPO involves four different models: <em>Actor</em>, <em>Critic</em>,
<em>Reference</em>, and <em>Reward</em>. Each model can have different functionalities
across a training iteration. For example, the <em>Actor</em> should first
<em>generate</em> responses given prompts and then be <em>trained</em> given rewards,
values, and KL regularizations.</p>
<p>Training iterations of PPO can be illustrated as follows:</p>
<img alt="Dataflow graph of RLHF." class="align-center" src="_images/rlhf_dfg.svg" /><p>We can see that there are six distinct <em>function calls</em> on these four
models. In ReaL, these function calls can have independent <em>allocations</em>
and <em>parallel strategies</em>. Each GPU can accommodate parameter shards of
multiple models (e.g., both the Actor and the Reward). Between two
function calls upon the same model, ReaL will automatically re-allocate
model parameters between source and destination locations and properly
remap parallel strategies.</p>
<p>In provided command, fields <code class="docutils literal notranslate"><span class="pre">actor</span></code>, <code class="docutils literal notranslate"><span class="pre">critic</span></code>, <code class="docutils literal notranslate"><span class="pre">ref</span></code>, and <code class="docutils literal notranslate"><span class="pre">rew</span></code>
specify the configurations of the four models. The allocations and
parallel strategies for function calls are automatically handled by the
<code class="docutils literal notranslate"><span class="pre">heuristic</span></code> allocation mode. This is a near-optimal execution strategy
found by the search engine in ReaL. Relative code can be found in the
<code class="docutils literal notranslate"><span class="pre">_heuristic_rpc_allocation</span></code> method of <a class="reference external" href="https://github.com/openpsi-project/ReaLHF/blob/main/realhf/experiments/common/ppo_exp.py">experiments/common/ppo_exp.py</a>.</p>
<p>For the details of PPO hyperparameters in the <code class="docutils literal notranslate"><span class="pre">ppo</span></code> field, please
check <a class="reference internal" href="expconfig.html#realhf.PPOHyperparameters" title="realhf.PPOHyperparameters"><code class="xref py py-class docutils literal notranslate"><span class="pre">realhf.PPOHyperparameters</span></code></a> for a detailed explanation.</p>
<img alt="_images/ppo_rwd.svg" class="align-center" src="_images/ppo_rwd.svg" /><div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">0: 20240618-14:46:38.007 master worker INFO: Epoch 1/1 step 39/39 (global step 39) finishes. ... Total time consumption: 574.312s. ...</span>
<span class="go">...</span>
<span class="go">0: 20240618-14:46:54.387 master worker INFO: Execution finished!</span>
<span class="go">0: 20240618-14:46:54.387 master worker INFO: Training complete! Yeah!!!</span>
</pre></div>
</div>
<p>We train PPO on 5000 prompts over 1 epoch, which consumes about 10
minutes. Summing up the time of the three stages, we can finish the RLHF
process <strong>within half an hour!</strong> This efficiency can largely help
algorithm developers to search for the best hyperparameters and iterate
on the algorithm design.</p>
<p>We also provide additional PPO examples to use <a class="reference external" href="https://github.com/openpsi-project/ReaLHF/blob/main/examples/scripts/local/ppo_symm.sh">symmetric
parallelization</a>
or <a class="reference external" href="https://github.com/openpsi-project/ReaLHF/blob/main/examples/scripts/local/ppo_minibatched.sh">mini-batched training</a>
when you encounter OOM issues.</p>
</section>
<section id="stage-4-evaluation">
<h3>Stage 4: Evaluation<a class="headerlink" href="#stage-4-evaluation" title="Link to this heading">¶</a></h3>
<p>After training, you can evaluate your trained model by generating
responses for a given task. Although it is usally convinient to use
external evaluation libraries, ReaL provides a build-in <a class="reference external" href="https://github.com/openpsi-project/ReaLHF/blob/main/examples/scripts/local/gen.sh">generation
script</a>.
It also uses CUDAGraph, similar to vLLM, to reduce kernel launch
overhead and accelerate the generation process. It’s up to the user to
decide whether to use the built-in script or external libraries.</p>
</section>
</section>
<section id="the-next-step">
<h2>The Next Step<a class="headerlink" href="#the-next-step" title="Link to this heading">¶</a></h2>
<p>You have now figured out how to run built-in experiments and how to
manage training hyperparameters, logs, and checkpoints within ReaL.</p>
<p>Next, you can follow the <a class="reference internal" href="distributed.html"><span class="doc">Setting Up Distributed Experiments</span></a> section to set up your
experiments in a large cluster, or proceed to the <a class="reference internal" href="customization.html"><span class="doc">Customization</span></a>
section to learn how to customize the datasets, models, and algorithms.
We provide examples for running PPO with reference EMA, the ReMax
algorithm, and the GRPO algorithm under the <a class="reference external" href="https://github.com/openpsi-project/ReaLHF/blob/main/examples/">examples/</a>
directory.</p>
<p>If you would like to deeply understand the implementation details of
ReaL, please refer to <a class="reference internal" href="impl.html"><span class="doc">Implementation Details</span></a> and <a class="reference internal" href="arch.html"><span class="doc">Code Architecture</span></a>.</p>
</section>
</section>

</article>
        <aside class="nftt-toc my-3">
          
          <div class="my-sm-1 my-lg-0 ps-xl-3 text-muted">
            <button class="btn btn-link link-dark p-lg-0 mb-2 mb-lg-0 text-decoration-none nftt-toc-toggle d-lg-none" type="button" data-bs-toggle="collapse" data-bs-target="#tocContents" aria-expanded="false" aria-controls="tocContents"
            >On this page <i class="ms-2 bi bi-chevron-expand"></i></button>
            <div class="title d-none d-lg-block">
              <i class="bi bi-file-earmark-text"></i>&nbsp;&nbsp;<span class="small">On this page</span>
            </div>
            <div class="collapse nftt-toc-collapse" id="tocContents">
              <nav id="TableOfContents">
                <ul>
<li><a class="reference internal" href="#">Quickstart</a><ul>
<li><a class="reference internal" href="#installation">Installation</a></li>
<li><a class="reference internal" href="#rlhf-with-4x-llama-7b-in-30min">RLHF with 4x LLaMA-7B in 30min</a><ul>
<li><a class="reference internal" href="#stage-1-supervised-fine-tuning">Stage 1: Supervised Fine-Tuning</a></li>
<li><a class="reference internal" href="#stage-2-1-reward-modeling-rm">Stage 2.1: Reward Modeling (RM)</a></li>
<li><a class="reference internal" href="#stage-2-2-direct-preference-optimization-dpo">Stage 2.2: Direct Preference Optimization (DPO)</a></li>
<li><a class="reference internal" href="#stage-3-ppo">Stage 3: PPO</a></li>
<li><a class="reference internal" href="#stage-4-evaluation">Stage 4: Evaluation</a></li>
</ul>
</li>
<li><a class="reference internal" href="#the-next-step">The Next Step</a></li>
</ul>
</li>
</ul>

              </nav>
            </div>
          </div>
          
        </aside>
      </div>
    </div>

    <footer class="nftt-footer">
      <nav id="paginator" class="py-4" aria-label="Documentation navigation">
    <div class="container">
      <ul class="pagination justify-content-between mb-0"><li class="d-flex page-item">
            <a href="expconfig.html" class="d-flex px-5 align-items-end" rel="prev" aria-label="Previous page: Configurations">
              <span class="prev-page"><i class="bi bi-caret-left"></i></span>
              <div class="d-none d-sm-flex flex-column">
                <span class="text-small text-start text-muted">Previous</span>
                <span class="underline">Configurations</span>
              </div>
            </a>
          </li>
        <li class="d-flex page-item ms-auto">
            <a href="distributed.html" class="d-flex px-5 align-items-end" rel="next" aria-label="Next page: Setting Up Distributed Experiments">
              <div class="d-flex flex-column">
                <span class="text-small text-end text-start text-muted">Next</span>
                <span class="underline">Setting Up Distributed Experiments</span>
              </div>
              <span class="next-page"><i class="bi bi-caret-right"></i></span>
            </a>
          </li>
        
      </ul>
    </div>
  </nav>

      <div class="py-5 px-4 px-md-3">
  <div class="container">
    

    <div class="row">
      <div class="col-lg-12 text-center">
        <a class="brand-text d-inline-flex align-items-center mb-2 text-decoration-none" href="/" aria-label="Nefertiti-for-Sphinx">
          <span class="fs-6 fw-bold">ReaL</span>
        </a>
        
          <ul class="list-unstyled small text-muted">
            <li>2024, Wei Fu & Zhiyu Mei</li>
          </ul>
        
        
        <div class="built-with pt-2">
          Built with <a href="http://sphinx-doc.org">Sphinx 7.3.7</a> and <a href="https://github.com/danirus/sphinx-nefertiti">Nefertiti 0.7.4</a>
        </div>
        
      </div>
    </div>
  </div>
</div>
    </footer>
    <script src="_static/documentation_options.js?v=e259d695"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/docs-versions.js?v=08b0cbfb"></script>
    <script src="_static/sphinx-nefertiti.min.js?v=de1d41e1"></script>
    <script src="_static/bootstrap.bundle.min.js?v=ff4e7878"></script>
  </body>
</html>